{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Training Notebook\n",
        "\n",
        "This notebook allows us to train our model using a GPU.\n",
        "\n",
        "**Before running this notebook, follow these steps:**\n",
        "\n",
        "1. In your Google Drive, go to MyDrive and create a folder `inf265_project_3`.\n",
        "2. Put the required files in this folder: `tokenizer.py`, `train.py`, `config.py` and `utils.py`.\n",
        "3. In the upper-right corner, click the down arrow and select `Change runtime type`.\n",
        "4. Choose `Runtime: Python3` and `Hardware accelerator: T4 GPU`. Do not select the `High-RAM` option.\n",
        "5. If required, click `Connect`. The bottom status bar should read something like `Connected to Python 3 Google Compute Engine backend (GPU)`.\n",
        "\n",
        "**Warning:** You get some free compute time every 24 hours. As long as you are connected to a GPU runtime, this will count towards your quota. If you are not training your model, make sure to click `Runtime >> Disconnect and delete runtime` so you don't waste your free compute.\n",
        "\n",
        "## Install Python Libraries\n",
        "\n",
        "Start by running the following cell to install required libraries:"
      ],
      "metadata": {
        "id": "-1_t2VH5lQ-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tokenizers"
      ],
      "metadata": {
        "id": "w0jMukGdk9XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Mounting Google Drive\n",
        "\n",
        "To save the tokenizer, model and optimizer checkpoints, we will mount Google Drive in the next code cell. Make sure you have created a directory `inf265_project_3` in your Google Drive under `MyDrive` and put your Python files there.\n",
        "\n",
        "We also use the `autoreload` Jupyter extension allowing us to re-import external files without restarting the kernel. This is useful if you need to do small changes in some Python files. You can find the files in the file browser (the folder icon in the left sidebar). Note that you need to mount your Google Drive before you can access the files from Colab. It might also take a few seconds before the file is updated after saving.\n",
        "\n",
        "Run the following cell to mount Google Drive and import the necessary files."
      ],
      "metadata": {
        "id": "ugs9vXVZND3E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ur7peL_ubgQp"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/inf265_project_3')\n",
        "\n",
        "from pathlib import Path\n",
        "from tokenizer import train_tokenizer\n",
        "from train import train_model\n",
        "from config import config\n",
        "from utils import print_config\n",
        "\n",
        "# Append paths to filenames for saving on Google Drive\n",
        "gdrive_base_path = \"/content/drive/MyDrive/inf265_project_3/\"\n",
        "\n",
        "if \"MyDrive\" not in config.tokenizer_filename: # Only append once\n",
        "  config.tokenizer_filename = gdrive_base_path + config.tokenizer_filename\n",
        "  config.model_filename = gdrive_base_path + config.model_filename\n",
        "  config.optimizer_filename = gdrive_base_path + config.optimizer_filename\n",
        "\n",
        "print_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Tokenizer\n",
        "\n",
        "Train and save the tokenizer. This might take a few minutes to complete. But you only have to do this once as it will save the tokenizer for later use."
      ],
      "metadata": {
        "id": "9oY_iGrVez67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not Path(config.tokenizer_filename).exists():\n",
        "  tokenizer = train_tokenizer(config)\n",
        "else:\n",
        "  print(f\"Tokenizer already exists at {config.tokenizer_filename}\")"
      ],
      "metadata": {
        "id": "NQmbaj1zm6Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Your Model\n",
        "\n",
        "We use the `train_model` function from `train.py`. This will save a model (and optimizer) checkpoint every 500 epochs. If you get disconnected or use all your daily compute, you can continue training again later.\n",
        "\n",
        "When you have trained your model for around 3-5 epochs, download the model and tokenizer files from Google Drive and put them in your local `temp` folder. Then you can use these when doing inference (text generation).\n",
        "\n",
        "A single epoch might take around 30 minutes to complete."
      ],
      "metadata": {
        "id": "Rb4D0vJgPrJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(config)"
      ],
      "metadata": {
        "id": "KL6PyeKnnp26"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}